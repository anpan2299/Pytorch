{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1q-yZ4edKo67",
    "outputId": "57105991-e9ae-4ce8-a056-5bcaf8dc6542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo MNIST\n"
     ]
    }
   ],
   "source": [
    "#cnn2.py - grad2020 - Testado em TF2 em Colab\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np; import sys\n",
    "def deslocaEsquerda(a):\n",
    "  d=a.copy(); d[:,0:-1]=a[:,1:]; return d\n",
    "def deslocaDireita(a):\n",
    "  d=a.copy(); d[:,1:]=a[:,0:-1]; return d\n",
    "def deslocaCima(a):\n",
    "  d=a.copy(); d[0:-1,:]=a[1:,:]; return d\n",
    "def deslocaBaixo(a):\n",
    "  d=a.copy(); d[1:,:]=a[0:-1,:]; return d\n",
    "print(\"Lendo MNIST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nb7MVqmSLc0U"
   },
   "outputs": [],
   "source": [
    "(AX, AY), (QX, QY) = mnist.load_data()\n",
    "AX=255-AX; QX=255-QX\n",
    "\n",
    "AX = np.resize(AX,(5*60000,28,28))\n",
    "AY = np.resize(AY,(5*60000,1))\n",
    "for s in range(60000):\n",
    "  AX[s+60000]=deslocaEsquerda(AX[s])\n",
    "  AX[s+2*60000]=deslocaDireita(AX[s])\n",
    "  AX[s+3*60000]=deslocaCima(AX[s])\n",
    "  AX[s+4*60000]=deslocaBaixo(AX[s])\n",
    "  AY[s+60000]=AY[s]\n",
    "  AY[s+2*60000]=AY[s]\n",
    "  AY[s+3*60000]=AY[s]\n",
    "  AY[s+4*60000]=AY[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59kAdk5IMvDi",
    "outputId": "cdcf9772-4c6b-4ae9-a1f7-6eb2ff494eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertendo para categorico e float\n"
     ]
    }
   ],
   "source": [
    "print(\"Convertendo para categorico e float\")\n",
    "nclasses = 10\n",
    "AY2 = keras.utils.to_categorical(AY, nclasses)\n",
    "QY2 = keras.utils.to_categorical(QY, nclasses)\n",
    "nl, nc = AX.shape[1], AX.shape[2] #28, 28\n",
    "AX = AX.astype('float32') / 255.0 - 0.5 # -0.5 a +0.5\n",
    "QX = QX.astype('float32') / 255.0 - 0.5 # -0.5 a +0.5\n",
    "AX = AX.reshape(AX.shape[0], nl, nc, 1)\n",
    "QX = QX.reshape(QX.shape[0], nl, nc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awf2nI05NDxV",
    "outputId": "8a2e42d8-e7de-47b2-8a5f-7b01e4574d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando modelo\n",
      "Epoch 1/100\n",
      "300/300 - 142s - loss: 0.4269 - accuracy: 0.8656 - val_loss: 0.0448 - val_accuracy: 0.9860 - lr: 0.0010 - 142s/epoch - 475ms/step\n",
      "Epoch 2/100\n",
      "300/300 - 142s - loss: 0.1114 - accuracy: 0.9701 - val_loss: 0.0303 - val_accuracy: 0.9905 - lr: 0.0010 - 142s/epoch - 472ms/step\n",
      "Epoch 3/100\n",
      "300/300 - 140s - loss: 0.0797 - accuracy: 0.9791 - val_loss: 0.0241 - val_accuracy: 0.9926 - lr: 0.0010 - 140s/epoch - 468ms/step\n",
      "Epoch 4/100\n",
      "300/300 - 141s - loss: 0.0644 - accuracy: 0.9833 - val_loss: 0.0249 - val_accuracy: 0.9919 - lr: 0.0010 - 141s/epoch - 469ms/step\n",
      "Epoch 5/100\n",
      "300/300 - 141s - loss: 0.0558 - accuracy: 0.9855 - val_loss: 0.0194 - val_accuracy: 0.9939 - lr: 0.0010 - 141s/epoch - 470ms/step\n",
      "Epoch 6/100\n",
      "300/300 - 140s - loss: 0.0495 - accuracy: 0.9870 - val_loss: 0.0164 - val_accuracy: 0.9949 - lr: 0.0010 - 140s/epoch - 466ms/step\n",
      "Epoch 7/100\n",
      "300/300 - 139s - loss: 0.0437 - accuracy: 0.9884 - val_loss: 0.0220 - val_accuracy: 0.9938 - lr: 0.0010 - 139s/epoch - 464ms/step\n",
      "Epoch 8/100\n",
      "300/300 - 140s - loss: 0.0399 - accuracy: 0.9893 - val_loss: 0.0180 - val_accuracy: 0.9950 - lr: 0.0010 - 140s/epoch - 468ms/step\n",
      "Epoch 9/100\n",
      "300/300 - 140s - loss: 0.0357 - accuracy: 0.9902 - val_loss: 0.0174 - val_accuracy: 0.9947 - lr: 0.0010 - 140s/epoch - 466ms/step\n",
      "Epoch 10/100\n",
      "300/300 - 139s - loss: 0.0332 - accuracy: 0.9912 - val_loss: 0.0173 - val_accuracy: 0.9948 - lr: 0.0010 - 139s/epoch - 463ms/step\n",
      "Epoch 11/100\n",
      "300/300 - 139s - loss: 0.0313 - accuracy: 0.9916 - val_loss: 0.0152 - val_accuracy: 0.9956 - lr: 0.0010 - 139s/epoch - 465ms/step\n",
      "Epoch 12/100\n",
      "300/300 - 141s - loss: 0.0297 - accuracy: 0.9919 - val_loss: 0.0157 - val_accuracy: 0.9952 - lr: 0.0010 - 141s/epoch - 469ms/step\n",
      "Epoch 13/100\n",
      "300/300 - 140s - loss: 0.0279 - accuracy: 0.9924 - val_loss: 0.0149 - val_accuracy: 0.9953 - lr: 0.0010 - 140s/epoch - 466ms/step\n",
      "Epoch 14/100\n",
      "300/300 - 139s - loss: 0.0263 - accuracy: 0.9928 - val_loss: 0.0160 - val_accuracy: 0.9955 - lr: 0.0010 - 139s/epoch - 464ms/step\n",
      "Epoch 15/100\n",
      "300/300 - 140s - loss: 0.0250 - accuracy: 0.9931 - val_loss: 0.0185 - val_accuracy: 0.9949 - lr: 0.0010 - 140s/epoch - 466ms/step\n",
      "Epoch 16/100\n",
      "300/300 - 138s - loss: 0.0237 - accuracy: 0.9933 - val_loss: 0.0155 - val_accuracy: 0.9959 - lr: 0.0010 - 138s/epoch - 461ms/step\n",
      "Epoch 17/100\n",
      "300/300 - 139s - loss: 0.0224 - accuracy: 0.9937 - val_loss: 0.0169 - val_accuracy: 0.9952 - lr: 0.0010 - 139s/epoch - 462ms/step\n",
      "Epoch 18/100\n",
      "300/300 - 139s - loss: 0.0221 - accuracy: 0.9937 - val_loss: 0.0178 - val_accuracy: 0.9953 - lr: 0.0010 - 139s/epoch - 463ms/step\n",
      "Epoch 19/100\n",
      "300/300 - 140s - loss: 0.0208 - accuracy: 0.9941 - val_loss: 0.0158 - val_accuracy: 0.9952 - lr: 0.0010 - 140s/epoch - 466ms/step\n",
      "Epoch 20/100\n",
      "300/300 - 139s - loss: 0.0193 - accuracy: 0.9947 - val_loss: 0.0151 - val_accuracy: 0.9961 - lr: 0.0010 - 139s/epoch - 463ms/step\n",
      "Epoch 21/100\n",
      "300/300 - 140s - loss: 0.0195 - accuracy: 0.9945 - val_loss: 0.0150 - val_accuracy: 0.9963 - lr: 0.0010 - 140s/epoch - 466ms/step\n",
      "Epoch 22/100\n",
      "300/300 - 140s - loss: 0.0186 - accuracy: 0.9949 - val_loss: 0.0150 - val_accuracy: 0.9962 - lr: 0.0010 - 140s/epoch - 467ms/step\n",
      "Epoch 23/100\n",
      "300/300 - 140s - loss: 0.0190 - accuracy: 0.9947 - val_loss: 0.0163 - val_accuracy: 0.9958 - lr: 0.0010 - 140s/epoch - 467ms/step\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "300/300 - 140s - loss: 0.0180 - accuracy: 0.9949 - val_loss: 0.0165 - val_accuracy: 0.9953 - lr: 0.0010 - 140s/epoch - 466ms/step\n",
      "Epoch 25/100\n",
      "300/300 - 140s - loss: 0.0169 - accuracy: 0.9953 - val_loss: 0.0139 - val_accuracy: 0.9957 - lr: 9.0000e-04 - 140s/epoch - 467ms/step\n",
      "Epoch 26/100\n",
      "300/300 - 140s - loss: 0.0158 - accuracy: 0.9953 - val_loss: 0.0144 - val_accuracy: 0.9961 - lr: 9.0000e-04 - 140s/epoch - 467ms/step\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "300/300 - 139s - loss: 0.0163 - accuracy: 0.9953 - val_loss: 0.0142 - val_accuracy: 0.9959 - lr: 9.0000e-04 - 139s/epoch - 463ms/step\n",
      "Epoch 28/100\n",
      "300/300 - 140s - loss: 0.0147 - accuracy: 0.9957 - val_loss: 0.0180 - val_accuracy: 0.9955 - lr: 8.1000e-04 - 140s/epoch - 467ms/step\n",
      "Epoch 29/100\n",
      "300/300 - 139s - loss: 0.0149 - accuracy: 0.9958 - val_loss: 0.0167 - val_accuracy: 0.9955 - lr: 8.1000e-04 - 139s/epoch - 465ms/step\n",
      "Epoch 30/100\n",
      "300/300 - 141s - loss: 0.0150 - accuracy: 0.9957 - val_loss: 0.0154 - val_accuracy: 0.9957 - lr: 8.1000e-04 - 141s/epoch - 469ms/step\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "300/300 - 141s - loss: 0.0145 - accuracy: 0.9957 - val_loss: 0.0162 - val_accuracy: 0.9957 - lr: 8.1000e-04 - 141s/epoch - 470ms/step\n",
      "Epoch 32/100\n",
      "300/300 - 139s - loss: 0.0133 - accuracy: 0.9961 - val_loss: 0.0195 - val_accuracy: 0.9951 - lr: 7.2900e-04 - 139s/epoch - 462ms/step\n",
      "Epoch 33/100\n",
      "300/300 - 139s - loss: 0.0126 - accuracy: 0.9963 - val_loss: 0.0170 - val_accuracy: 0.9957 - lr: 7.2900e-04 - 139s/epoch - 463ms/step\n",
      "Epoch 34/100\n",
      "300/300 - 140s - loss: 0.0131 - accuracy: 0.9962 - val_loss: 0.0155 - val_accuracy: 0.9961 - lr: 7.2900e-04 - 140s/epoch - 467ms/step\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "300/300 - 139s - loss: 0.0127 - accuracy: 0.9962 - val_loss: 0.0181 - val_accuracy: 0.9952 - lr: 7.2900e-04 - 139s/epoch - 462ms/step\n",
      "Epoch 36/100\n",
      "300/300 - 139s - loss: 0.0120 - accuracy: 0.9965 - val_loss: 0.0168 - val_accuracy: 0.9958 - lr: 6.5610e-04 - 139s/epoch - 463ms/step\n",
      "Epoch 37/100\n",
      "300/300 - 140s - loss: 0.0117 - accuracy: 0.9967 - val_loss: 0.0177 - val_accuracy: 0.9960 - lr: 6.5610e-04 - 140s/epoch - 468ms/step\n",
      "Epoch 38/100\n",
      "300/300 - 139s - loss: 0.0113 - accuracy: 0.9967 - val_loss: 0.0168 - val_accuracy: 0.9959 - lr: 6.5610e-04 - 139s/epoch - 464ms/step\n",
      "Epoch 39/100\n",
      "300/300 - 139s - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.0172 - val_accuracy: 0.9961 - lr: 6.5610e-04 - 139s/epoch - 463ms/step\n",
      "Epoch 40/100\n",
      "300/300 - 138s - loss: 0.0110 - accuracy: 0.9967 - val_loss: 0.0158 - val_accuracy: 0.9961 - lr: 6.5610e-04 - 138s/epoch - 460ms/step\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "300/300 - 139s - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.0167 - val_accuracy: 0.9962 - lr: 6.5610e-04 - 139s/epoch - 464ms/step\n",
      "Epoch 42/100\n",
      "300/300 - 138s - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.0173 - val_accuracy: 0.9961 - lr: 5.9049e-04 - 138s/epoch - 461ms/step\n",
      "Epoch 43/100\n",
      "300/300 - 138s - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0181 - val_accuracy: 0.9956 - lr: 5.9049e-04 - 138s/epoch - 461ms/step\n",
      "Epoch 44/100\n",
      "300/300 - 139s - loss: 0.0110 - accuracy: 0.9968 - val_loss: 0.0146 - val_accuracy: 0.9964 - lr: 5.9049e-04 - 139s/epoch - 465ms/step\n",
      "Epoch 45/100\n",
      "300/300 - 138s - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0177 - val_accuracy: 0.9963 - lr: 5.9049e-04 - 138s/epoch - 461ms/step\n",
      "Epoch 46/100\n",
      "300/300 - 138s - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.0150 - val_accuracy: 0.9964 - lr: 5.9049e-04 - 138s/epoch - 460ms/step\n",
      "Epoch 47/100\n",
      "300/300 - 139s - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0183 - val_accuracy: 0.9962 - lr: 5.9049e-04 - 139s/epoch - 464ms/step\n",
      "Epoch 48/100\n",
      "300/300 - 139s - loss: 0.0094 - accuracy: 0.9972 - val_loss: 0.0185 - val_accuracy: 0.9959 - lr: 5.9049e-04 - 139s/epoch - 463ms/step\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "300/300 - 138s - loss: 0.0098 - accuracy: 0.9970 - val_loss: 0.0178 - val_accuracy: 0.9963 - lr: 5.9049e-04 - 138s/epoch - 459ms/step\n",
      "Epoch 50/100\n",
      "300/300 - 140s - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.0179 - val_accuracy: 0.9960 - lr: 5.3144e-04 - 140s/epoch - 465ms/step\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "300/300 - 138s - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.0169 - val_accuracy: 0.9961 - lr: 5.3144e-04 - 138s/epoch - 460ms/step\n",
      "Epoch 52/100\n",
      "300/300 - 142s - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0202 - val_accuracy: 0.9957 - lr: 4.7830e-04 - 142s/epoch - 472ms/step\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "300/300 - 139s - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.0191 - val_accuracy: 0.9961 - lr: 4.7830e-04 - 139s/epoch - 464ms/step\n",
      "Epoch 54/100\n",
      "300/300 - 140s - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.0172 - val_accuracy: 0.9961 - lr: 4.3047e-04 - 140s/epoch - 465ms/step\n",
      "Epoch 55/100\n",
      "300/300 - 141s - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.0171 - val_accuracy: 0.9964 - lr: 4.3047e-04 - 141s/epoch - 470ms/step\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
      "300/300 - 141s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0170 - val_accuracy: 0.9964 - lr: 4.3047e-04 - 141s/epoch - 468ms/step\n",
      "Epoch 57/100\n",
      "300/300 - 139s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0179 - val_accuracy: 0.9966 - lr: 3.8742e-04 - 139s/epoch - 464ms/step\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
      "300/300 - 139s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0164 - val_accuracy: 0.9964 - lr: 3.8742e-04 - 139s/epoch - 465ms/step\n",
      "Epoch 59/100\n",
      "300/300 - 139s - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.0182 - val_accuracy: 0.9965 - lr: 3.4868e-04 - 139s/epoch - 463ms/step\n",
      "Epoch 60/100\n",
      "300/300 - 139s - loss: 0.0078 - accuracy: 0.9978 - val_loss: 0.0189 - val_accuracy: 0.9960 - lr: 3.4868e-04 - 139s/epoch - 463ms/step\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
      "300/300 - 140s - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0193 - val_accuracy: 0.9958 - lr: 3.4868e-04 - 140s/epoch - 466ms/step\n",
      "Epoch 62/100\n",
      "300/300 - 138s - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0182 - val_accuracy: 0.9957 - lr: 3.1381e-04 - 138s/epoch - 461ms/step\n",
      "Epoch 63/100\n",
      "300/300 - 139s - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.0172 - val_accuracy: 0.9961 - lr: 3.1381e-04 - 139s/epoch - 463ms/step\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
      "300/300 - 139s - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0170 - val_accuracy: 0.9964 - lr: 3.1381e-04 - 139s/epoch - 465ms/step\n",
      "Epoch 65/100\n",
      "300/300 - 138s - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.0190 - val_accuracy: 0.9962 - lr: 2.8243e-04 - 138s/epoch - 460ms/step\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
      "300/300 - 138s - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0186 - val_accuracy: 0.9957 - lr: 2.8243e-04 - 138s/epoch - 461ms/step\n",
      "Epoch 67/100\n",
      "300/300 - 140s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0177 - val_accuracy: 0.9962 - lr: 2.5419e-04 - 140s/epoch - 465ms/step\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n",
      "300/300 - 138s - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0182 - val_accuracy: 0.9962 - lr: 2.5419e-04 - 138s/epoch - 460ms/step\n",
      "Epoch 69/100\n",
      "300/300 - 139s - loss: 0.0069 - accuracy: 0.9980 - val_loss: 0.0178 - val_accuracy: 0.9964 - lr: 2.2877e-04 - 139s/epoch - 465ms/step\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n",
      "300/300 - 138s - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.0172 - val_accuracy: 0.9958 - lr: 2.2877e-04 - 138s/epoch - 460ms/step\n",
      "Epoch 71/100\n",
      "300/300 - 138s - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.0182 - val_accuracy: 0.9963 - lr: 2.0589e-04 - 138s/epoch - 462ms/step\n",
      "Epoch 72/100\n",
      "300/300 - 139s - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0176 - val_accuracy: 0.9966 - lr: 2.0589e-04 - 139s/epoch - 464ms/step\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n",
      "300/300 - 139s - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0188 - val_accuracy: 0.9962 - lr: 2.0589e-04 - 139s/epoch - 465ms/step\n",
      "Epoch 74/100\n",
      "300/300 - 140s - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0181 - val_accuracy: 0.9962 - lr: 1.8530e-04 - 140s/epoch - 468ms/step\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n",
      "300/300 - 139s - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0180 - val_accuracy: 0.9961 - lr: 1.8530e-04 - 139s/epoch - 462ms/step\n",
      "Epoch 76/100\n",
      "300/300 - 138s - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0180 - val_accuracy: 0.9964 - lr: 1.6677e-04 - 138s/epoch - 460ms/step\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.00015009464841568844.\n",
      "300/300 - 140s - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.0174 - val_accuracy: 0.9960 - lr: 1.6677e-04 - 140s/epoch - 466ms/step\n",
      "Epoch 78/100\n",
      "300/300 - 138s - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0186 - val_accuracy: 0.9958 - lr: 1.5009e-04 - 138s/epoch - 461ms/step\n",
      "Epoch 79/100\n",
      "300/300 - 140s - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0187 - val_accuracy: 0.9962 - lr: 1.5009e-04 - 140s/epoch - 466ms/step\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 0.0001350851875031367.\n",
      "300/300 - 139s - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0198 - val_accuracy: 0.9960 - lr: 1.5009e-04 - 139s/epoch - 464ms/step\n",
      "Epoch 81/100\n",
      "300/300 - 139s - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0183 - val_accuracy: 0.9963 - lr: 1.3509e-04 - 139s/epoch - 465ms/step\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 0.00012157666351413355.\n",
      "300/300 - 139s - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0192 - val_accuracy: 0.9958 - lr: 1.3509e-04 - 139s/epoch - 462ms/step\n",
      "Epoch 83/100\n",
      "300/300 - 140s - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.0197 - val_accuracy: 0.9961 - lr: 1.2158e-04 - 140s/epoch - 466ms/step\n",
      "Epoch 84/100\n",
      "300/300 - 139s - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0194 - val_accuracy: 0.9963 - lr: 1.2158e-04 - 139s/epoch - 462ms/step\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 0.00010941899454337544.\n",
      "300/300 - 139s - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0194 - val_accuracy: 0.9963 - lr: 1.2158e-04 - 139s/epoch - 464ms/step\n",
      "Epoch 86/100\n",
      "300/300 - 138s - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.0183 - val_accuracy: 0.9963 - lr: 1.0942e-04 - 138s/epoch - 460ms/step\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "300/300 - 140s - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0191 - val_accuracy: 0.9962 - lr: 1.0942e-04 - 140s/epoch - 466ms/step\n",
      "Epoch 88/100\n",
      "300/300 - 139s - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.0185 - val_accuracy: 0.9963 - lr: 1.0000e-04 - 139s/epoch - 462ms/step\n",
      "Epoch 89/100\n",
      "300/300 - 138s - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.0201 - val_accuracy: 0.9959 - lr: 1.0000e-04 - 138s/epoch - 462ms/step\n",
      "Epoch 90/100\n",
      "300/300 - 141s - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0187 - val_accuracy: 0.9962 - lr: 1.0000e-04 - 141s/epoch - 470ms/step\n",
      "Epoch 91/100\n",
      "300/300 - 141s - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0198 - val_accuracy: 0.9963 - lr: 1.0000e-04 - 141s/epoch - 471ms/step\n",
      "Epoch 92/100\n",
      "300/300 - 140s - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0196 - val_accuracy: 0.9961 - lr: 1.0000e-04 - 140s/epoch - 466ms/step\n",
      "Epoch 93/100\n",
      "300/300 - 139s - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.0189 - val_accuracy: 0.9963 - lr: 1.0000e-04 - 139s/epoch - 464ms/step\n",
      "Epoch 94/100\n",
      "300/300 - 141s - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0189 - val_accuracy: 0.9961 - lr: 1.0000e-04 - 141s/epoch - 470ms/step\n",
      "Epoch 95/100\n",
      "300/300 - 139s - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0193 - val_accuracy: 0.9960 - lr: 1.0000e-04 - 139s/epoch - 463ms/step\n",
      "Epoch 96/100\n",
      "300/300 - 140s - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0200 - val_accuracy: 0.9961 - lr: 1.0000e-04 - 140s/epoch - 468ms/step\n",
      "Epoch 97/100\n",
      "300/300 - 139s - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0197 - val_accuracy: 0.9961 - lr: 1.0000e-04 - 139s/epoch - 464ms/step\n",
      "Epoch 98/100\n",
      "300/300 - 140s - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.0195 - val_accuracy: 0.9963 - lr: 1.0000e-04 - 140s/epoch - 466ms/step\n",
      "Epoch 99/100\n",
      "300/300 - 140s - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.0188 - val_accuracy: 0.9964 - lr: 1.0000e-04 - 140s/epoch - 467ms/step\n",
      "Epoch 100/100\n",
      "300/300 - 141s - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0185 - val_accuracy: 0.9961 - lr: 1.0000e-04 - 141s/epoch - 470ms/step\n",
      "Test loss: 0.0185\n",
      "Test accuracy: 99.61 %\n",
      "Test error: 0.39 %\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(20, kernel_size=(3,3), activation='relu', input_shape=(nl,nc,1))) #28 - 3 +1 = 26\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) #13\n",
    "model.add(Conv2D(40, kernel_size=(3,3), activation='relu')) #13 - 3 +1 = 1\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) #7\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nclasses, activation='softmax'))\n",
    "print(\"Treinando modelo\")\n",
    "opt=optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='accuracy',\n",
    " factor=0.9, patience=2, min_lr=0.0001, verbose=True)\n",
    "model.fit(AX, AY2, batch_size=1000, epochs=100, verbose=2,\n",
    " validation_data=(QX, QY2), callbacks=[reduce_lr])\n",
    "score = model.evaluate(QX, QY2, verbose=False)\n",
    "print('Test loss: %.4f'%(score[0]))\n",
    "print('Test accuracy: %.2f %%'%(100*score[1]))\n",
    "print('Test error: %.2f %%'%(100*(1-score[1])))\n",
    "model.save(\"cnn2.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_grey = 0.1307\n",
    "stdd_gray = 0.3081\n",
    "\n",
    "transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((mean_grey),(stdd_gray))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms,\n",
    "                           download=True)\n",
    "test_dataset = datasets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (train_dataset[20][0].numpy())\n",
    "#plt.imshow(img.reshape(28,28), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.05945994\n",
      "   1.9941516  -0.10600711 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.25874594 -0.10600711 -0.29693064\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296  1.0013492\n",
      "   2.783302    1.1668162  -0.37330002 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296  1.2559141   2.783302    1.0140774\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.3522093\n",
      "   2.783302    2.783302    0.03400347 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296  1.2559141   2.783302    2.6305633\n",
      "   0.58131754 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.64495873\n",
      "   2.783302    2.783302    0.03400347 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296  1.2559141   2.783302    2.783302\n",
      "   2.6051068   0.23765521 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.27147415  2.0959773\n",
      "   2.783302    2.783302    1.1286315  -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296  0.8358821   2.783302    2.783302\n",
      "   2.783302    1.8795974  -0.20783298 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.1994705   2.783302\n",
      "   2.783302    2.783302    1.1286315  -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.38602826  1.1668162   2.783302\n",
      "   2.783302    2.783302    0.8486104  -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.09327888  2.350542\n",
      "   2.783302    2.783302    0.03400347 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.23328947  2.3250856\n",
      "   2.783302    2.783302    2.2105315  -0.1823765  -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.1994705   2.783302\n",
      "   2.783302    2.783302    0.03400347 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296  1.5741198\n",
      "   2.783302    2.783302    2.783302    0.18674228 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.34784356  0.9631645   0.9631645   2.0450644   2.783302\n",
      "   2.783302    2.783302    0.03400347 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.8486104\n",
      "   2.783302    2.783302    2.783302    0.91225153 -0.42421296  0.3140246\n",
      "   1.0522621   1.2050011   2.783302    2.783302    2.783302    2.783302\n",
      "   2.783302    2.2741728  -0.1823765  -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   1.6632175   2.7960303   2.7960303   2.7960303   2.7960303   2.7960303\n",
      "   2.7960303   2.8214867   2.7960303   2.7960303   2.7960303   2.7960303\n",
      "   2.7960303   2.7960303   0.8358821  -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   0.1994705   2.783302    2.783302    2.783302    2.783302    2.783302\n",
      "   2.783302    2.7960303   2.783302    2.783302    2.783302    2.783302\n",
      "   2.783302    2.783302    1.5486634  -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   0.1994705   2.783302    2.783302    2.783302    2.783302    2.783302\n",
      "   2.783302    2.3378139   2.3250856   1.3704681   1.1795444   2.783302\n",
      "   2.783302    2.783302    1.5486634  -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   0.1994705   2.783302    2.783302    2.783302    2.5541937   2.1723468\n",
      "   0.70859987 -0.42421296 -0.42421296 -0.42421296  0.1994705   2.783302\n",
      "   2.783302    2.783302    1.5486634  -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.2460177   1.5868481   2.019608    1.4977505   0.14855757 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.1994705   2.783302\n",
      "   2.783302    2.783302    2.4396398  -0.20783298 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.1994705   2.783302\n",
      "   2.783302    2.783302    2.783302   -0.13146357 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.00418122  2.4778244\n",
      "   2.783302    2.783302    2.783302    1.5741198  -0.37330002 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.2758399\n",
      "   2.4905527   2.783302    2.783302    2.783302   -0.2842024  -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   0.25038344  2.5287373   2.783302    2.783302    0.37766576 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296  0.7213281   2.1978033   1.2431858  -0.2842024  -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]]\n"
     ]
    }
   ],
   "source": [
    "print(img.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100   \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, kernel_size_1):\n",
    "        super(CNN,self).__init__()\n",
    "        #output size = (in - kernel_size + 2*padding)/stride + 1 = 28- 3 +2*1 +1 =28\n",
    "        self.cnn1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = kernel_size_1,stride = 1, padding = 1)\n",
    "        self.batchnorm = nn.BatchNorm2d(8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.cnn2 = nn.Conv2d(in_channels = 8, out_channels = 32, kernel_size = 5,stride = 1, padding = 2)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        #32x7x7 = 1568\n",
    "        self.fc1 = nn.Linear(1568,600)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.fc2 = nn.Linear(600,10)\n",
    "    def forward(self, x): \n",
    "        x1 = self.cnn1(x)\n",
    "        x1 = self.batchnorm(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.maxpool(x1)\n",
    "        \n",
    "        x2 = self.cnn2(x1)\n",
    "        x2 = self.batchnorm2(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x2 = self.maxpool(x2)\n",
    "        #flatten\n",
    "        x2 = x2.view(batch_size, 1568)\n",
    "        \n",
    "        out = self.fc1(x2)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(kernel_size_1 = 3)\n",
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.835, Training Accuracy: 93.127%, Test Loss: 0.061, Test Accuracy: 97.940%\n",
      "Epoch [2/10], Training Loss: 0.065, Training Accuracy: 98.073%, Test Loss: 0.051, Test Accuracy: 98.450%\n",
      "Epoch [3/10], Training Loss: 0.053, Training Accuracy: 98.303%, Test Loss: 0.037, Test Accuracy: 98.890%\n",
      "Epoch [4/10], Training Loss: 0.049, Training Accuracy: 98.500%, Test Loss: 0.044, Test Accuracy: 98.670%\n",
      "Epoch [5/10], Training Loss: 0.052, Training Accuracy: 98.442%, Test Loss: 0.048, Test Accuracy: 98.660%\n",
      "Epoch [6/10], Training Loss: 0.061, Training Accuracy: 98.217%, Test Loss: 0.058, Test Accuracy: 98.340%\n",
      "Epoch [7/10], Training Loss: 0.066, Training Accuracy: 98.165%, Test Loss: 0.061, Test Accuracy: 98.400%\n",
      "Epoch [8/10], Training Loss: 0.079, Training Accuracy: 97.960%, Test Loss: 0.074, Test Accuracy: 98.410%\n",
      "Epoch [9/10], Training Loss: 0.079, Training Accuracy: 98.075%, Test Loss: 0.098, Test Accuracy: 97.890%\n",
      "Epoch [10/10], Training Loss: 0.098, Training Accuracy: 97.688%, Test Loss: 0.075, Test Accuracy: 98.090%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 \n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    iter_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)                 # Difference between the actual and predicted (loss function)\n",
    "        iter_loss += loss.item()\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()                                   # Backpropagation\n",
    "        optimizer.step()  \n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        correct += (predicted == labels).sum().item()\n",
    "        iterations += 1\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    train_accuracy.append(100*correct/len(train_dataset))\n",
    "    \n",
    "    testing_loss = 0.0\n",
    "    correct = 0.0\n",
    "    iterations = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        if CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)                 # Difference between the actual and predicted (loss function)\n",
    "        testing_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        correct += (predicted == labels).sum().item()\n",
    "        iterations += 1\n",
    "    test_loss.append(testing_loss/iterations)\n",
    "    test_accuracy.append(100*correct/len(test_dataset))\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.3f}, Training Accuracy: {:.3f}%, Test Loss: {:.3f}, Test Accuracy: {:.3f}%'.format\n",
    "          (epoch+1, epochs, train_loss[-1], train_accuracy[-1], test_loss[-1], test_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (10,10))\n",
    "plt.plot(train_loss,label= \"train_loss\")\n",
    "plt.plot(test_loss, label= \"test_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (10,10))\n",
    "plt.plot(train_accuracy,label= \"train_accuracy\")\n",
    "plt.plot(test_accuracy, label= \"test_accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
